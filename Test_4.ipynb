{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjdfF7DQfsXmUBoz6DBSVi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaison7733/Jaison_Meta_Scifor_Technology/blob/ML/Test_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm questions"
      ],
      "metadata": {
        "id": "Ja7P5gpMQ1Gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " Why is feature scaling important in gradient descent?\n",
        "\n",
        "Feature scaling is a crucial preprocessing step in machine learning, particularly when using gradient descent-based algorithms like linear regression or logistic regression. It involves transforming the features of your dataset to a common scale, typically between 0 and 1 or standardizing them to have zero mean and unit variance.\n",
        "\n",
        "Feature scaling is important in gradient descent because\n",
        "\n",
        "\n",
        "1.Faster Convergence:\n",
        "\n",
        "Balanced Gradient Updates: When features have different scales, gradient descent can take longer to converge. This is because features with larger scales can dominate the gradient updates, causing the algorithm to take larger steps in those directions and smaller steps in directions of features with smaller scales.\n",
        "Smoother Contour Plot: Scaling features leads to a more circular contour plot, making it easier for gradient descent to find the optimal solution. Without scaling, the contour plot can be elongated, leading to a zigzagging path towards the minimum.\n",
        "\n",
        "\n",
        "2.Improved Model Performance:\n",
        "\n",
        "Reduced Bias: Features with larger scales can have a disproportionate impact on the model's predictions. Scaling ensures that all features contribute equally to the model's learning process.\n",
        "Better Generalization: A well-scaled model is more likely to generalize well to unseen data, as it is less sensitive to the specific scale of the training data.\n",
        "\n",
        "\n",
        "3.Numerical Stability:\n",
        "\n",
        "Avoiding Overflow or Underflow: Some algorithms, like those involving exponentiation, can be prone to numerical issues if features have very large or very small values. Scaling helps prevent these issues.\n"
      ],
      "metadata": {
        "id": "nTHZhzpxSRzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Solving"
      ],
      "metadata": {
        "id": "ZAL00nU6Quu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a dataset with missing values, how would you handle them before training an ML model?\n",
        "\n",
        "1.Deletion:\n",
        "\n",
        "Row-wise Deletion: Remove entire rows containing missing values. This is suitable when the missing values are few and randomly distributed.\n",
        "Column-wise Deletion: Remove entire columns with many missing values. This should be done cautiously, as it can lead to loss of valuable information.\n",
        "\n",
        "2.Imputation:\n",
        "\n",
        "Mean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the respective feature. This is a simple but effective method for numerical data.\n",
        "\n",
        "K-Nearest Neighbors (KNN) Imputation: Impute missing values using the values of the k nearest neighbors. This method considers the relationships between features.\n",
        "\n",
        "Multiple Imputation: Create multiple complete datasets by imputing missing values with different plausible values. This can account for uncertainty in the imputation process.\n",
        "\n",
        "Regression Imputation: Use a regression model to predict missing values based on other features.\n",
        "\n",
        "\n",
        "\n",
        "3.Feature Engineering:\n",
        "\n",
        "Create a New Feature: Create a new binary feature indicating whether a value is missing or not. This can capture patterns in missingness.\n",
        "\n",
        "Use Missingness as a Category: For categorical features, treat missing values as a separate category."
      ],
      "metadata": {
        "id": "jnnZ72oSUQSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design a pipeline for building a classification model. Include steps for data preprocessing.\n",
        "\n",
        "Pipeline for Building a Classification Model\n",
        "\n",
        "1.Data Collection and Exploration\n",
        "\n",
        "Data Sources: Identify and gather relevant data from various sources like databases, APIs, or public datasets.\n",
        "\n",
        "Data Exploration:\n",
        "Initial Inspection: Check data types, missing values, outliers, and inconsistencies.\n",
        "\n",
        "Statistical Summary: Calculate summary statistics (mean, median, mode, standard deviation, etc.) for numerical features.\n",
        "\n",
        "Data Visualization: Create visualizations (histograms, box plots, scatter plots) to understand data distribution and relationships between features.\n",
        "\n",
        "2.Data Preprocessing\n",
        "\n",
        "Handling Missing Values:\n",
        "Deletion: Remove rows or columns with excessive missing values.\n",
        "\n",
        "Imputation: Fill missing values with statistical measures (mean, median, mode) or predictive models.\n",
        "\n",
        "Outlier Detection and Handling:\n",
        "\n",
        "Statistical Methods: Identify outliers using Z-scores or IQR.\n",
        "\n",
        "Visualization: Use box plots or scatter plots to visually identify outliers.\n",
        "Handling:\n",
        "\n",
        "Trimming: Remove outliers.\n",
        "\n",
        "Capping: Replace outliers with a defined threshold.\n",
        "\n",
        "Winsorization: Replace outliers with a percentile value.\n",
        "\n",
        "Feature Engineering:\n",
        "\n",
        "Feature Creation: Create new features by combining or transforming existing ones.\n",
        "\n",
        "Feature Selection: Identify the most relevant features using techniques like correlation analysis, feature importance, or dimensionality reduction.\n",
        "\n",
        "Data Normalization/Standardization:\n",
        "\n",
        "Normalization: Scale features to a specific range (e.g., 0-1).\n",
        "\n",
        "Standardization: Scale features to have zero mean and unit variance.\n",
        "\n",
        "3.Data Splitting\n",
        "\n",
        "Train-Test Split: Divide the dataset into training and testing sets.\n",
        "\n",
        "Stratified Split: Ensure that the class distribution in the training and testing sets is similar to the original dataset.\n",
        "\n",
        "4.Model Selection and Training\n",
        "\n",
        "Choose a Model: Select an appropriate classification algorithm based on the problem and dataset characteristics (e.g., Logistic Regression, Decision Trees, Random Forest, Support Vector Machines, Neural Networks).\n",
        "\n",
        "Hyperparameter Tuning: Optimize model performance by tuning hyperparameters using techniques like grid search or random search.\n",
        "\n",
        "Model Training: Train the selected model on the training data.\n",
        "\n",
        "5.Model Evaluation\n",
        "\n",
        "Performance Metrics: Evaluate the model's performance using metrics like accuracy, precision, recall, F1-score, and confusion matrix.\n",
        "\n",
        "Cross-Validation: Assess the model's generalization ability by evaluating it on multiple folds of the data.\n",
        "\n",
        "6.Model Deployment\n",
        "\n",
        "Model Serialization: Save the trained model for future use.\n",
        "\n",
        "Deployment Platform: Choose a suitable platform (e.g., cloud platforms, web frameworks) to deploy the model.\n",
        "\n",
        "API Creation: Create an API to expose the model's predictions to other applications.\n",
        "\n",
        "7.Model Monitoring and Retraining\n",
        "\n",
        "Monitor Performance: Continuously monitor the model's performance on new data\n",
        "Retrain Model: Retrain the model periodically or when performance degrades significantly."
      ],
      "metadata": {
        "id": "_lpXhdPBV6bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding"
      ],
      "metadata": {
        "id": "g9TBebKTH9DL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python script to implement a decision tree classifier using Scikit-learn."
      ],
      "metadata": {
        "id": "Lsy4EdvURRcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wECvRRlRboO",
        "outputId": "be5e148b-ed4c-4b1d-c2bc-d2ff4e454960"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a dataset, write code to split the data into training and testing sets using an 80-20 split."
      ],
      "metadata": {
        "id": "swFxVYtCSNpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "UQz8uHJJTIxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case Study"
      ],
      "metadata": {
        "id": "vAJ-PjspTP7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A company wants to predict employee attrition. What kind of ML problem is this? Which algorithms would you choose and why?\n",
        "\n",
        "Predicting Employee Attrition is a Classification Problem.\n",
        "\n",
        "Problem Type:\n",
        "* Classification: Categorizing employees into two classes: those who will stay and those who will leave.\n",
        "\n",
        "Algorithm Selection:\n",
        "\n",
        "* Logistic Regression: Simple, interpretable, and efficient.\n",
        "* Decision Trees: Handles both numerical and categorical data, generates human-readable rules.\n",
        "* Random Forest: Reduces overfitting, handles missing values and outliers well, provides feature importance.\n"
      ],
      "metadata": {
        "id": "bJI906XWTTM5"
      }
    }
  ]
}